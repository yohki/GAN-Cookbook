{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN Cookbook: DCGAN (2)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yohki/GAN-Cookbook/blob/master/GAN_Cookbook_DCGAN_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zg0H49VtWbm",
        "colab_type": "text"
      },
      "source": [
        "# Generator\n",
        "\n",
        "今までのGANとDCGANを切り替えて使えるようにしているのでコードが多少長くなっている。DCGANのモデルは34〜54行目。\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1C2Z03je-tWywYQMhAXfqzTt6SJGvDwLQ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "798BZsTStXzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.layers import Dense, Reshape, Input, BatchNormalization\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.convolutional import UpSampling2D, Convolution2D, MaxPooling2D,Deconvolution2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam, SGD, Nadam,Adamax\n",
        "from keras import initializers\n",
        "from keras.utils import plot_model\n",
        "\n",
        "class Generator(object):\n",
        "    def __init__(self, width = 28, height= 28, channels = 1, latent_size=100, model_type = 'simple'):\n",
        "        \n",
        "        self.W = width\n",
        "        self.H = height\n",
        "        self.C = channels\n",
        "        self.LATENT_SPACE_SIZE = latent_size\n",
        "        self.latent_space = np.random.normal(0,1,(self.LATENT_SPACE_SIZE,))\n",
        "\n",
        "        if model_type=='simple':\n",
        "            self.Generator = self.model()\n",
        "            self.OPTIMIZER = Adam(lr=0.0002, decay=8e-9)\n",
        "            self.Generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
        "        elif model_type=='DCGAN':\n",
        "            self.Generator = self.dc_model()\n",
        "            self.OPTIMIZER = Adam(lr=1e-4, beta_1=0.2)\n",
        "            self.Generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER,metrics=['accuracy'])\n",
        "        self.save_model()\n",
        "        self.summary()\n",
        "        \n",
        "    # DCGANのモデル\n",
        "    def dc_model(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256*8*8,activation=LeakyReLU(0.2), input_dim=self.LATENT_SPACE_SIZE))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Reshape((8, 8, 256)))\n",
        "        model.add(UpSampling2D())\n",
        "\n",
        "        model.add(Convolution2D(128, 5, 5, border_mode='same',activation=LeakyReLU(0.2)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(UpSampling2D())\n",
        "\n",
        "        model.add(Convolution2D(64, 5, 5, border_mode='same',activation=LeakyReLU(0.2)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(UpSampling2D())\n",
        "\n",
        "        model.add(Convolution2D(self.C, 5, 5, border_mode='same', activation='tanh'))\n",
        "        \n",
        "        return model\n",
        "\n",
        "    # 普通のGANモデル\n",
        "    def model(self, block_starting_size=128,num_blocks=4):\n",
        "        model = Sequential()\n",
        "        \n",
        "        block_size = block_starting_size \n",
        "        model.add(Dense(block_size, input_shape=(self.LATENT_SPACE_SIZE,)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "        for i in range(num_blocks-1):\n",
        "            block_size = block_size * 2\n",
        "            model.add(Dense(block_size))\n",
        "            model.add(LeakyReLU(alpha=0.2))\n",
        "            model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "        model.add(Dense(self.W * self.H * self.C, activation='tanh'))\n",
        "        model.add(Reshape((self.W, self.H, self.C)))\n",
        "        \n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.Generator.summary()\n",
        "\n",
        "    def save_model(self):\n",
        "        plot_model(self.Generator.model, to_file='DCGAN/Generator_Model.png')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8mQkdTweMt7",
        "colab_type": "text"
      },
      "source": [
        "* l.38:最初の入力は16384個のノイズ。 \n",
        "* l.41: それを8x8x256の3次元ボリュームに変換。\n",
        "* l.42: 縦横2倍にアップスケールして16x16x256にする。\n",
        "* l.44: 16x16x256の入力に対して、5x5x256のフィルタを使って畳み込みを行い、16x16の特徴マップを128個生成している。 \n",
        "* 通常畳み込みを行うと出力サイズが小さくなるが、`border_mode='same'`は周りを0で埋めることでサイズを変えずに畳み込みを行うことができる（ゼロパディング）。\n",
        "\n",
        "![alt text](https://deepage.net/img/convolutional_neural_network/zero_padding.jpg\n",
        ")\n",
        "\n",
        "* l.46: 再び縦横2倍にアップスケールしてサイズを32x32x128にする。\n",
        "* l.48: 2回目の畳み込み。32x32x128の入力に対してフィルタサイズが5x5x128、フィルタ数が64なので出力は32x32x64になる。\n",
        "* l.50: 再度アップサンプリング。これでサイズが64x64x64になる。\n",
        "* l.52: 最終画像として出力するために畳み込みを行う。入力サイズ=64x64x64、フィルタサイズ=5x5x64、フィルタ数=3、出力サイズ=64x64x3。これで64x64のRGB画像ができる。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkXUW-Cipf_G",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1hP6PuCuwHefJhvbnTmJ5t1qyZqM17f5B)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rFeqkg6pfE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Lambda, concatenate\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam, SGD,Nadam, Adamax\n",
        "import keras.backend as K\n",
        "from keras.utils import plot_model\n",
        "\n",
        "\n",
        "class Discriminator(object):\n",
        "    def __init__(self, width = 28, height= 28, channels = 1, latent_size=100,model_type = 'simple'):\n",
        "        self.W = width\n",
        "        self.H = height\n",
        "        self.C = channels\n",
        "        self.CAPACITY = width*height*channels\n",
        "        self.SHAPE = (width,height,channels)\n",
        "        \n",
        "        if model_type=='simple':\n",
        "            self.Discriminator = self.model()\n",
        "            self.OPTIMIZER = Adam(lr=0.0002, decay=8e-9)\n",
        "            self.Discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
        "        elif model_type=='DCGAN':\n",
        "            self.Discriminator = self.dc_model()\n",
        "            self.OPTIMIZER = Adam(lr=1e-4, beta_1=0.2)\n",
        "            self.Discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
        "\n",
        "        self.save_model()\n",
        "        self.summary()\n",
        "\n",
        "    def dc_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Convolution2D(64, 5, 5, subsample=(2,2), input_shape=(self.W,self.H,self.C), border_mode='same',activation=LeakyReLU(alpha=0.2)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Convolution2D(128, 5, 5, subsample=(2,2), border_mode='same',activation=LeakyReLU(alpha=0.2)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        return model\n",
        "\n",
        "    def model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=self.SHAPE))\n",
        "        model.add(Dense(self.CAPACITY, input_shape=self.SHAPE))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(int(self.CAPACITY/2)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.Discriminator.summary()\n",
        "\n",
        "    def save_model(self):\n",
        "        plot_model(self.Discriminator.model, to_file='DCGAN/Discriminator_Model.png')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn9aG_4SXLEa",
        "colab_type": "text"
      },
      "source": [
        "- `Convolution2D`の`subsample=(2,2)`はストライドのことと思われる。`Conv2D(... stride=(2,2))`と同等で、これによりダウンサンプリングが行われる。\n",
        "- 2層目の`Convolution2D`では`subsample=(2,2)`としておきつつ、`border_mode='same'`として出力サイズを同じに保っているがなぜ？\n",
        "- l.38, 41: Batch Normalizationは入れた方がいい説とそうでない説がある。（→[KerasでDCGAN書く](https://qiita.com/t-ae/items/236457c29ba85a7579d5)）\n",
        "- l.37, 40: `Dropout()`は一定割合で結合を切ってやることで過学習を防ぐ処理。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp_55kIeq1kE",
        "colab_type": "text"
      },
      "source": [
        "# GAN\n",
        "\n",
        "前の章で使っていたものと同じ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IavDME86pvlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import plot_model\n",
        "\n",
        "class GAN(object):\n",
        "    def __init__(self,discriminator,generator):\n",
        "        self.OPTIMIZER = SGD(lr=2e-4,nesterov=True)\n",
        "        self.Generator = generator\n",
        "\n",
        "        self.Discriminator = discriminator\n",
        "        self.Discriminator.trainable = False\n",
        "        \n",
        "        self.gan_model = self.model()\n",
        "        self.gan_model.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
        "        self.save_model()\n",
        "        self.summary()\n",
        "\n",
        "    def model(self):\n",
        "        model = Sequential()\n",
        "        model.add(self.Generator)\n",
        "        model.add(self.Discriminator)\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.gan_model.summary()\n",
        "\n",
        "    def save_model(self):\n",
        "        plot_model(self.gan_model.model, to_file='DCGAN/GAN_Model.png')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOMUK9UDsF2O",
        "colab_type": "text"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2QMfypkq69V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import time\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, width = 28, height= 28, channels = 1, latent_size=100, epochs =50000, batch=32, checkpoint=50,model_type=-1,data_path = ''):\n",
        "        self.W = width\n",
        "        self.H = height\n",
        "        self.C = channels\n",
        "        self.EPOCHS = epochs\n",
        "        self.BATCH = batch\n",
        "        self.CHECKPOINT = checkpoint\n",
        "        self.model_type=model_type\n",
        "\n",
        "        self.LATENT_SPACE_SIZE = latent_size\n",
        "\n",
        "        self.generator = Generator(height=self.H, width=self.W, channels=self.C, latent_size=self.LATENT_SPACE_SIZE,model_type = 'DCGAN')\n",
        "        self.discriminator = Discriminator(height=self.H, width=self.W, channels=self.C,model_type = 'DCGAN')\n",
        "        self.gan = GAN(generator=self.generator.Generator, discriminator=self.discriminator.Discriminator)\n",
        "\n",
        "        #self.load_MNIST()\n",
        "        self.load_npy(data_path)\n",
        "\n",
        "    def load_npy(self,npy_path):\n",
        "        self.X_train = np.load(npy_path)\n",
        "        self.X_train = self.X_train[:int(0.25*float(len(self.X_train)))]\n",
        "        self.X_train = (self.X_train.astype(np.float32) - 127.5)/127.5\n",
        "        self.X_train = np.expand_dims(self.X_train, axis=3)\n",
        "        return\n",
        "\n",
        "    def load_MNIST(self,model_type=3):\n",
        "        allowed_types = [-1,0,1,2,3,4,5,6,7,8,9]\n",
        "        if self.model_type not in allowed_types:\n",
        "            print('ERROR: Only Integer Values from -1 to 9 are allowed')\n",
        "\n",
        "        (self.X_train, self.Y_train), (_, _) = mnist.load_data()\n",
        "        if self.model_type!=-1:\n",
        "            self.X_train = self.X_train[np.where(self.Y_train==int(self.model_type))[0]]\n",
        "        \n",
        "        # Rescale -1 to 1\n",
        "        # Find Normalize Function from CV Class  \n",
        "        self.X_train = ( np.float32(self.X_train) - 127.5) / 127.5\n",
        "        self.X_train = np.expand_dims(self.X_train, axis=3)\n",
        "        return\n",
        "\n",
        "    def train(self):\n",
        "        for e in range(self.EPOCHS):\n",
        "            e_start = time.time()\n",
        "            b = 0\n",
        "            X_train_temp = deepcopy(self.X_train)\n",
        "            while self.BATCH < len(X_train_temp):\n",
        "                b_start = time.time()\n",
        "                # Keep track of Batches\n",
        "                b=b+1\n",
        "\n",
        "                # Train Discriminator\n",
        "                # Make the training batch for this model be half real, half noise\n",
        "                # Grab Real Images for this training batch\n",
        "                if self.flipCoin():\n",
        "                    count_real_images = int(self.BATCH)\n",
        "                    starting_index = randint(0, (len(X_train_temp)-count_real_images))\n",
        "                    real_images_raw = X_train_temp[ starting_index : (starting_index + count_real_images) ]\n",
        "                    #self.plot_check_batch(b,real_images_raw)\n",
        "                    # Delete the images used until we have none left\n",
        "                    X_train_temp = np.delete(X_train_temp,range(starting_index,(starting_index + count_real_images)),0)\n",
        "                    x_batch = real_images_raw.reshape( count_real_images, self.W, self.H, self.C )\n",
        "                    y_batch = np.ones([count_real_images,1])\n",
        "                else:\n",
        "                    # Grab Generated Images for this training batch\n",
        "                    latent_space_samples = self.sample_latent_space(self.BATCH)\n",
        "                    x_batch = self.generator.Generator.predict(latent_space_samples)\n",
        "                    y_batch = np.zeros([self.BATCH,1])\n",
        "\n",
        "                # Now, train the discriminator with this batch\n",
        "                discriminator_loss = self.discriminator.Discriminator.train_on_batch(x_batch,y_batch)[0]\n",
        "            \n",
        "                # In practice, flipping the label when training the generator improves convergence\n",
        "                if self.flipCoin(chance=0.9):\n",
        "                    y_generated_labels = np.ones([self.BATCH,1])\n",
        "                else:\n",
        "                    y_generated_labels = np.zeros([self.BATCH,1])\n",
        "                x_latent_space_samples = self.sample_latent_space(self.BATCH)\n",
        "                generator_loss = self.gan.gan_model.train_on_batch(x_latent_space_samples,y_generated_labels)\n",
        "    \n",
        "                b_elapsed = time.time() - b_start\n",
        "                if b % self.CHECKPOINT == 0:\n",
        "                    print('Batch: ' + str(int(b)) + \n",
        "                       ', [Discriminator :: Loss: ' + str(discriminator_loss) + \n",
        "                       '], [ Generator :: Loss: '+str(generator_loss) + \n",
        "                       '], {0}s'.format(b_elapsed))\n",
        "                    label = str(e)+'_'+str(b)\n",
        "                    self.plot_checkpoint(label)\n",
        "            e_elapsed = time.time() - e_start\n",
        "            print ('Epoch: '+str(int(e)) + \n",
        "                   ', [Discriminator :: Loss: ' + str(discriminator_loss) + \n",
        "                   '], [ Generator :: Loss: ' + str(generator_loss) + \n",
        "                   '], {0}s'.format(e_elapsed))\n",
        "                        \n",
        "            if e % self.CHECKPOINT == 0 :\n",
        "                self.plot_checkpoint(e)\n",
        "        return\n",
        "\n",
        "    def flipCoin(self,chance=0.5):\n",
        "        return np.random.binomial(1, chance)\n",
        "\n",
        "    def sample_latent_space(self, instances):\n",
        "        return np.random.normal(0, 1, (instances,self.LATENT_SPACE_SIZE))\n",
        "\n",
        "    def plot_checkpoint(self,e):\n",
        "        filename = \"DCGAN/sample_20190709_\"+str(e)+\".png\"\n",
        "\n",
        "        noise = self.sample_latent_space(16)\n",
        "        images = self.generator.Generator.predict(noise)\n",
        "        \n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            if self.C==1:\n",
        "                image = images[i, :, :]\n",
        "                image = np.reshape(image, [self.H,self.W])\n",
        "                image = (255*(image - np.min(image))/np.ptp(image)).astype(int)\n",
        "                plt.imshow(image,cmap='gray')\n",
        "            elif self.C==3:\n",
        "                image = images[i, :, :, :]\n",
        "                image = np.reshape(image, [self.H,self.W,self.C])\n",
        "                image = (255*(image - np.min(image))/np.ptp(image)).astype(int)\n",
        "                plt.imshow(image)\n",
        "            \n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(filename)\n",
        "        plt.close('all')\n",
        "        return\n",
        "\n",
        "    def plot_check_batch(self,b,images):\n",
        "        filename = \"DCGAN/batch_check_\"+str(b)+\".png\"\n",
        "        subplot_size = int(np.sqrt(images.shape[0]))\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(subplot_size, subplot_size, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.H,self.W,self.C])\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(filename)\n",
        "        plt.close('all')\n",
        "        return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IntquHndo8s-",
        "colab_type": "text"
      },
      "source": [
        "* l.53: 最初に`X_train_temp`に本物の画像を全部コピーしておく。\n",
        "* これまではバッチサイズ（32とか128とか）のうち半分を本物の画像の中からランダムに選択したもの、もう半分はGeneratorで生成した画像として、それらを一つにまとめた学習用データとして学習させていた。\n",
        "* 今回はバッチサイズ（128）の分の本物画像、もしくは生成画像だけを取ってきて学習を行うことを、本物の画像データがなくなるまで繰り返す。これによってパフォーマンス向上と、ネットワークが発散してしまうことを防げるらしい。\n",
        "* l.81: Generatorの学習の際に、10%だけ逆のラベル（＝偽物）を与えて学習させている。これにより収束しやすくなるらしい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDPKSlhvscEj",
        "colab_type": "text"
      },
      "source": [
        "# 学習実行\n",
        "\n",
        "バッチサイズとエポック数の関係については\n",
        "- [機械学習／ディープラーニングにおけるバッチサイズ、イテレーション数、エポック数の決め方](https://qiita.com/kenta1984/items/bad75a37d552510e4682)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEIqxobMsqmj",
        "colab_type": "code",
        "outputId": "01d3a00c-20b3-454b-d68a-2a48caec8ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "%cd /content\n",
        "from google.colab import drive\n",
        "drive.mount('./gdrive', force_remount=True)\n",
        "\n",
        "!mkdir -p '/content/gdrive/My Drive/Colab Notebooks/DCGAN'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at ./gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9pfdpDKsIME",
        "colab_type": "code",
        "outputId": "87a6a823-441c-42ec-f580-9595b5416f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd '/content/gdrive/My Drive/Colab Notebooks/'\n",
        "\n",
        "HEIGHT  = 64\n",
        "WIDTH   = 64\n",
        "CHANNEL = 3\n",
        "LATENT_SPACE_SIZE = 100\n",
        "EPOCHS = 100\n",
        "BATCH = 128\n",
        "CHECKPOINT = 100\n",
        "PATH = \"lsun/data/church_outdoor_train_lmdb_color.npy\"\n",
        "\n",
        "trainer = Trainer(height=HEIGHT,\\\n",
        "                 width=WIDTH,\\\n",
        "                 channels=CHANNEL,\\\n",
        "                 latent_size=LATENT_SPACE_SIZE,\\\n",
        "                 epochs =EPOCHS,\\\n",
        "                 batch=BATCH,\\\n",
        "                 checkpoint=CHECKPOINT,\\\n",
        "                 model_type='DCGAN',\\\n",
        "                 data_path=PATH)\n",
        "                 \n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0709 10:21:39.939268 140526303909760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n",
            "W0709 10:21:39.985949 140526303909760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0709 10:21:39.993033 140526303909760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0709 10:21:40.085930 140526303909760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0709 10:21:40.115274 140526303909760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), activation=<keras.lay..., padding=\"same\")`\n",
            "W0709 10:21:40.162531 140526303909760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0709 10:21:43.024029 140526303909760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), activation=<keras.lay..., padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (5, 5), activation=\"tanh\", padding=\"same\")`\n",
            "W0709 10:21:43.305959 140526303909760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0709 10:21:43.315991 140526303909760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:110: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), input_shape=(64, 64, 3..., activation=<keras.lay..., strides=(2, 2), padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n",
            "W0709 10:21:51.378436 140526303909760 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 16384)             1654784   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16384)             65536     \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 128)       819328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 64, 64, 3)         4803      \n",
            "=================================================================\n",
            "Total params: 2,750,083\n",
            "Trainable params: 2,716,931\n",
            "Non-trainable params: 33,152\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), activation=<keras.lay..., strides=(2, 2), padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:110: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 64)        4864      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 128)       204928    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 32769     \n",
            "=================================================================\n",
            "Total params: 243,329\n",
            "Trainable params: 242,945\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:110: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_1 (Sequential)    (None, 64, 64, 3)         2750083   \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 1)                 243329    \n",
            "=================================================================\n",
            "Total params: 2,993,412\n",
            "Trainable params: 2,716,931\n",
            "Non-trainable params: 276,481\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 100, [Discriminator :: Loss: 0.053489164], [ Generator :: Loss: 3.4269974], 0.5037744045257568s\n",
            "Batch: 200, [Discriminator :: Loss: 0.059224688], [ Generator :: Loss: 0.036105946], 0.4137582778930664s\n",
            "Batch: 300, [Discriminator :: Loss: 0.04643163], [ Generator :: Loss: 4.143644], 0.16156768798828125s\n",
            "Batch: 400, [Discriminator :: Loss: 0.08800068], [ Generator :: Loss: 3.7530365], 0.21278905868530273s\n",
            "Epoch: 0, [Discriminator :: Loss: 0.03809631], [ Generator :: Loss: 4.8829665], 141.56810069084167s\n",
            "Batch: 100, [Discriminator :: Loss: 0.03384541], [ Generator :: Loss: 3.704365], 0.5010769367218018s\n",
            "Batch: 200, [Discriminator :: Loss: 0.08427818], [ Generator :: Loss: 4.2868404], 0.4234747886657715s\n",
            "Batch: 300, [Discriminator :: Loss: 0.041978415], [ Generator :: Loss: 4.3665094], 0.32532548904418945s\n",
            "Batch: 400, [Discriminator :: Loss: 0.01184687], [ Generator :: Loss: 4.294094], 0.2246706485748291s\n",
            "Epoch: 1, [Discriminator :: Loss: 0.0047239666], [ Generator :: Loss: 4.849663], 132.55727791786194s\n",
            "Batch: 100, [Discriminator :: Loss: 0.22512875], [ Generator :: Loss: 1.497761], 0.5128052234649658s\n",
            "Batch: 200, [Discriminator :: Loss: 0.016361877], [ Generator :: Loss: 5.1092396], 0.16203069686889648s\n",
            "Batch: 300, [Discriminator :: Loss: 0.01913809], [ Generator :: Loss: 4.752839], 0.34563231468200684s\n",
            "Batch: 400, [Discriminator :: Loss: 0.004037567], [ Generator :: Loss: 6.8712716], 0.16162323951721191s\n",
            "Batch: 500, [Discriminator :: Loss: 0.0069851456], [ Generator :: Loss: 0.0023560496], 0.16253113746643066s\n",
            "Epoch: 2, [Discriminator :: Loss: 0.09679506], [ Generator :: Loss: 5.913659], 140.4314079284668s\n",
            "Batch: 100, [Discriminator :: Loss: 0.035744727], [ Generator :: Loss: 4.9915085], 0.16246461868286133s\n",
            "Batch: 200, [Discriminator :: Loss: 0.016368218], [ Generator :: Loss: 4.5986285], 0.38563084602355957s\n",
            "Batch: 300, [Discriminator :: Loss: 0.009284634], [ Generator :: Loss: 4.9018846], 0.2694218158721924s\n",
            "Batch: 400, [Discriminator :: Loss: 0.085340984], [ Generator :: Loss: 6.506858], 0.16169381141662598s\n",
            "Epoch: 3, [Discriminator :: Loss: 0.06292219], [ Generator :: Loss: 5.680567], 128.66025829315186s\n",
            "Batch: 100, [Discriminator :: Loss: 0.39540958], [ Generator :: Loss: 4.011325], 0.508415699005127s\n",
            "Batch: 200, [Discriminator :: Loss: 0.014484368], [ Generator :: Loss: 6.6082416], 0.1615283489227295s\n",
            "Batch: 300, [Discriminator :: Loss: 0.07723072], [ Generator :: Loss: 0.0061804275], 0.337536096572876s\n",
            "Batch: 400, [Discriminator :: Loss: 0.007180794], [ Generator :: Loss: 6.2914953], 0.2267765998840332s\n",
            "Epoch: 4, [Discriminator :: Loss: 0.012999629], [ Generator :: Loss: 6.0909557], 140.53919124603271s\n",
            "Batch: 100, [Discriminator :: Loss: 0.017707491], [ Generator :: Loss: 7.205375], 0.5122947692871094s\n",
            "Batch: 200, [Discriminator :: Loss: 0.097230405], [ Generator :: Loss: 6.0590334], 0.41454100608825684s\n",
            "Batch: 300, [Discriminator :: Loss: 0.0113100475], [ Generator :: Loss: 4.6081285], 0.31827330589294434s\n",
            "Batch: 400, [Discriminator :: Loss: 0.004332135], [ Generator :: Loss: 6.0655813], 0.23093748092651367s\n",
            "Batch: 500, [Discriminator :: Loss: 0.010730773], [ Generator :: Loss: 8.51844], 0.1529853343963623s\n",
            "Epoch: 5, [Discriminator :: Loss: 0.0015591043], [ Generator :: Loss: 7.1558247], 145.19615578651428s\n",
            "Batch: 100, [Discriminator :: Loss: 0.015145271], [ Generator :: Loss: 5.415727], 0.5275592803955078s\n",
            "Batch: 200, [Discriminator :: Loss: 0.006591845], [ Generator :: Loss: 7.606826], 0.16867971420288086s\n",
            "Batch: 300, [Discriminator :: Loss: 0.0037681842], [ Generator :: Loss: 8.596547], 0.16020488739013672s\n",
            "Batch: 400, [Discriminator :: Loss: 0.012401717], [ Generator :: Loss: 6.102687], 0.15949702262878418s\n",
            "Epoch: 6, [Discriminator :: Loss: 0.016037703], [ Generator :: Loss: 9.390995], 135.8995623588562s\n",
            "Batch: 100, [Discriminator :: Loss: 0.011448315], [ Generator :: Loss: 6.884837], 0.1602792739868164s\n",
            "Batch: 200, [Discriminator :: Loss: 0.031105138], [ Generator :: Loss: 10.190235], 0.42378759384155273s\n",
            "Batch: 300, [Discriminator :: Loss: 0.06457805], [ Generator :: Loss: 4.954482], 0.1599726676940918s\n",
            "Batch: 400, [Discriminator :: Loss: 0.015883347], [ Generator :: Loss: 9.380297], 0.24050354957580566s\n",
            "Batch: 500, [Discriminator :: Loss: 0.008910769], [ Generator :: Loss: 0.010535072], 0.13843965530395508s\n",
            "Epoch: 7, [Discriminator :: Loss: 0.0075628376], [ Generator :: Loss: 7.5666666], 145.08257412910461s\n",
            "Batch: 100, [Discriminator :: Loss: 0.01276837], [ Generator :: Loss: 6.9431496], 0.5019371509552002s\n",
            "Batch: 200, [Discriminator :: Loss: 0.035698842], [ Generator :: Loss: 5.113509], 0.4080994129180908s\n",
            "Batch: 300, [Discriminator :: Loss: 0.006485001], [ Generator :: Loss: 2.6631024], 0.3046271800994873s\n",
            "Batch: 400, [Discriminator :: Loss: 0.007183919], [ Generator :: Loss: 10.077551], 0.22086429595947266s\n",
            "Batch: 500, [Discriminator :: Loss: 0.021311237], [ Generator :: Loss: 8.8246355], 0.12931180000305176s\n",
            "Epoch: 8, [Discriminator :: Loss: 0.021311237], [ Generator :: Loss: 8.8246355], 142.93068027496338s\n",
            "Batch: 100, [Discriminator :: Loss: 0.03576106], [ Generator :: Loss: 4.4497633], 0.5152134895324707s\n",
            "Batch: 200, [Discriminator :: Loss: 0.0010144953], [ Generator :: Loss: 0.0016583853], 0.16167497634887695s\n",
            "Batch: 300, [Discriminator :: Loss: 0.16290557], [ Generator :: Loss: 0.86027825], 0.32708096504211426s\n",
            "Batch: 400, [Discriminator :: Loss: 0.026323862], [ Generator :: Loss: 5.782846], 0.23249363899230957s\n",
            "Batch: 500, [Discriminator :: Loss: 0.096601814], [ Generator :: Loss: 5.114382], 0.16314125061035156s\n",
            "Epoch: 9, [Discriminator :: Loss: 0.088672444], [ Generator :: Loss: 5.9705257], 146.12716031074524s\n",
            "Batch: 100, [Discriminator :: Loss: 0.006926324], [ Generator :: Loss: 7.179229], 0.16318964958190918s\n",
            "Batch: 200, [Discriminator :: Loss: 0.05174469], [ Generator :: Loss: 5.271705], 0.4349379539489746s\n",
            "Batch: 300, [Discriminator :: Loss: 0.061198853], [ Generator :: Loss: 4.0430217], 0.33808279037475586s\n",
            "Batch: 400, [Discriminator :: Loss: 0.00870859], [ Generator :: Loss: 4.6991544], 0.22674250602722168s\n",
            "Batch: 500, [Discriminator :: Loss: 0.01605562], [ Generator :: Loss: 7.325529], 0.13057231903076172s\n",
            "Epoch: 10, [Discriminator :: Loss: 0.056755625], [ Generator :: Loss: 8.0722685], 143.74023938179016s\n",
            "Batch: 100, [Discriminator :: Loss: 0.0032465023], [ Generator :: Loss: 8.507341], 0.16153597831726074s\n",
            "Batch: 200, [Discriminator :: Loss: 3.5092978], [ Generator :: Loss: 4.5493603], 0.3926103115081787s\n",
            "Batch: 300, [Discriminator :: Loss: 0.0051307874], [ Generator :: Loss: 0.109309696], 0.2794201374053955s\n",
            "Batch: 400, [Discriminator :: Loss: 0.018462788], [ Generator :: Loss: 8.064644], 0.18612265586853027s\n",
            "Epoch: 11, [Discriminator :: Loss: 0.009787476], [ Generator :: Loss: 5.0492525], 137.30022311210632s\n",
            "Batch: 100, [Discriminator :: Loss: 0.03391759], [ Generator :: Loss: 7.6429963], 0.5241401195526123s\n",
            "Batch: 200, [Discriminator :: Loss: 0.016321909], [ Generator :: Loss: 5.9696727], 0.16260671615600586s\n",
            "Batch: 300, [Discriminator :: Loss: 0.005027936], [ Generator :: Loss: 0.008771499], 0.3401310443878174s\n",
            "Batch: 400, [Discriminator :: Loss: 0.021803083], [ Generator :: Loss: 1.4333851], 0.24686908721923828s\n",
            "Batch: 500, [Discriminator :: Loss: 0.017493283], [ Generator :: Loss: 5.619545], 0.1460280418395996s\n",
            "Epoch: 12, [Discriminator :: Loss: 0.05419914], [ Generator :: Loss: 0.12336613], 145.47075080871582s\n",
            "Batch: 100, [Discriminator :: Loss: 0.061908077], [ Generator :: Loss: 4.2639174], 0.16303443908691406s\n",
            "Batch: 200, [Discriminator :: Loss: 0.0013986086], [ Generator :: Loss: 8.24586], 0.15980935096740723s\n",
            "Batch: 300, [Discriminator :: Loss: 0.18256909], [ Generator :: Loss: 3.794459], 0.32926273345947266s\n",
            "Batch: 400, [Discriminator :: Loss: 0.020452656], [ Generator :: Loss: 3.4394393], 0.24862241744995117s\n",
            "Batch: 500, [Discriminator :: Loss: 0.061139386], [ Generator :: Loss: 3.4834433], 0.15017485618591309s\n",
            "Epoch: 13, [Discriminator :: Loss: 0.00672914], [ Generator :: Loss: 3.7742682], 147.61049103736877s\n",
            "Batch: 100, [Discriminator :: Loss: 0.014467686], [ Generator :: Loss: 7.732628], 0.5108132362365723s\n",
            "Batch: 200, [Discriminator :: Loss: 0.018351622], [ Generator :: Loss: 0.31509417], 0.40758371353149414s\n",
            "Batch: 300, [Discriminator :: Loss: 0.28480655], [ Generator :: Loss: 0.30368352], 0.32843971252441406s\n",
            "Batch: 400, [Discriminator :: Loss: 0.066180564], [ Generator :: Loss: 4.522639], 0.16355371475219727s\n",
            "Batch: 500, [Discriminator :: Loss: 0.020553436], [ Generator :: Loss: 5.416874], 0.1636204719543457s\n",
            "Epoch: 14, [Discriminator :: Loss: 0.049637467], [ Generator :: Loss: 0.019995037], 147.49908566474915s\n",
            "Batch: 100, [Discriminator :: Loss: 0.0128105655], [ Generator :: Loss: 6.7172537], 0.16325616836547852s\n",
            "Batch: 200, [Discriminator :: Loss: 0.07878054], [ Generator :: Loss: 3.780366], 0.41054296493530273s\n",
            "Batch: 300, [Discriminator :: Loss: 0.024091026], [ Generator :: Loss: 5.8063555], 0.16166901588439941s\n",
            "Batch: 400, [Discriminator :: Loss: 0.09258367], [ Generator :: Loss: 6.911051], 0.19132328033447266s\n",
            "Epoch: 15, [Discriminator :: Loss: 0.009919425], [ Generator :: Loss: 5.92216], 138.09354901313782s\n",
            "Batch: 100, [Discriminator :: Loss: 0.047418777], [ Generator :: Loss: 5.4107194], 0.518519401550293s\n",
            "Batch: 200, [Discriminator :: Loss: 0.008752279], [ Generator :: Loss: 5.8664365], 0.1626889705657959s\n",
            "Batch: 300, [Discriminator :: Loss: 0.38108635], [ Generator :: Loss: 3.3221529], 0.16286659240722656s\n",
            "Batch: 400, [Discriminator :: Loss: 0.0079493895], [ Generator :: Loss: 4.719781], 0.20677423477172852s\n",
            "Epoch: 16, [Discriminator :: Loss: 0.030169634], [ Generator :: Loss: 5.6153784], 138.23902797698975s\n",
            "Batch: 100, [Discriminator :: Loss: 0.01384377], [ Generator :: Loss: 5.494279], 0.16207456588745117s\n",
            "Batch: 200, [Discriminator :: Loss: 0.017215844], [ Generator :: Loss: 5.8103476], 0.4215729236602783s\n",
            "Batch: 300, [Discriminator :: Loss: 0.0037906547], [ Generator :: Loss: 7.449689], 0.16632723808288574s\n",
            "Batch: 400, [Discriminator :: Loss: 0.016375842], [ Generator :: Loss: 0.012585709], 0.2521023750305176s\n",
            "Batch: 500, [Discriminator :: Loss: 0.013242186], [ Generator :: Loss: 5.509679], 0.16315484046936035s\n",
            "Epoch: 17, [Discriminator :: Loss: 0.030020457], [ Generator :: Loss: 0.39175007], 147.62644290924072s\n",
            "Batch: 100, [Discriminator :: Loss: 0.0097487485], [ Generator :: Loss: 5.549608], 0.16262006759643555s\n",
            "Batch: 200, [Discriminator :: Loss: 0.022703003], [ Generator :: Loss: 0.016537735], 0.16585898399353027s\n",
            "Batch: 300, [Discriminator :: Loss: 0.051953185], [ Generator :: Loss: 4.1295843], 0.29659056663513184s\n",
            "Batch: 400, [Discriminator :: Loss: 0.028032593], [ Generator :: Loss: 5.9617367], 0.16037487983703613s\n",
            "Epoch: 18, [Discriminator :: Loss: 0.011422608], [ Generator :: Loss: 2.9482307], 139.8588035106659s\n",
            "Batch: 100, [Discriminator :: Loss: 0.018723102], [ Generator :: Loss: 4.213928], 0.5229010581970215s\n",
            "Batch: 200, [Discriminator :: Loss: 0.04247735], [ Generator :: Loss: 4.846009], 0.16183018684387207s\n",
            "Batch: 300, [Discriminator :: Loss: 0.038291913], [ Generator :: Loss: 5.295353], 0.1612393856048584s\n",
            "Batch: 400, [Discriminator :: Loss: 0.005985508], [ Generator :: Loss: 0.71207374], 0.20713257789611816s\n",
            "Epoch: 19, [Discriminator :: Loss: 0.016966326], [ Generator :: Loss: 4.9597273], 137.7095081806183s\n",
            "Batch: 100, [Discriminator :: Loss: 0.0195956], [ Generator :: Loss: 4.2570157], 0.5079870223999023s\n",
            "Batch: 200, [Discriminator :: Loss: 0.015083862], [ Generator :: Loss: 5.716476], 0.16125917434692383s\n",
            "Batch: 300, [Discriminator :: Loss: 0.019937621], [ Generator :: Loss: 0.6892529], 0.29143285751342773s\n",
            "Batch: 400, [Discriminator :: Loss: 0.03881233], [ Generator :: Loss: 3.3442116], 0.18589091300964355s\n",
            "Epoch: 20, [Discriminator :: Loss: 0.08830378], [ Generator :: Loss: 5.1105905], 134.71700859069824s\n",
            "Batch: 100, [Discriminator :: Loss: 0.045405276], [ Generator :: Loss: 5.7478967], 0.5107333660125732s\n",
            "Batch: 200, [Discriminator :: Loss: 0.02030566], [ Generator :: Loss: 0.0105093485], 0.1613295078277588s\n",
            "Batch: 300, [Discriminator :: Loss: 0.051786855], [ Generator :: Loss: 5.611474], 0.16174077987670898s\n",
            "Batch: 400, [Discriminator :: Loss: 0.018265136], [ Generator :: Loss: 6.1162343], 0.16111493110656738s\n",
            "Batch: 500, [Discriminator :: Loss: 0.018990206], [ Generator :: Loss: 5.8012877], 0.16519951820373535s\n",
            "Epoch: 21, [Discriminator :: Loss: 0.0069950423], [ Generator :: Loss: 4.4594746], 146.3638572692871s\n",
            "Batch: 100, [Discriminator :: Loss: 0.024345903], [ Generator :: Loss: 6.4656363], 0.16710686683654785s\n",
            "Batch: 200, [Discriminator :: Loss: 0.020987745], [ Generator :: Loss: 7.049596], 0.16267132759094238s\n",
            "Batch: 300, [Discriminator :: Loss: 0.021610435], [ Generator :: Loss: 0.02131488], 0.16169023513793945s\n",
            "Batch: 400, [Discriminator :: Loss: 0.0035098284], [ Generator :: Loss: 4.4299645], 0.20449519157409668s\n",
            "Epoch: 22, [Discriminator :: Loss: 0.017255986], [ Generator :: Loss: 3.6294165], 139.08374047279358s\n",
            "Batch: 100, [Discriminator :: Loss: 2.0318918], [ Generator :: Loss: 10.130714], 0.16278839111328125s\n",
            "Batch: 200, [Discriminator :: Loss: 0.032686416], [ Generator :: Loss: 6.895157], 0.15997886657714844s\n",
            "Batch: 300, [Discriminator :: Loss: 0.019305017], [ Generator :: Loss: 5.3895373], 0.29677700996398926s\n",
            "Batch: 400, [Discriminator :: Loss: 0.1327951], [ Generator :: Loss: 0.02653691], 0.19751644134521484s\n",
            "Epoch: 23, [Discriminator :: Loss: 0.028860925], [ Generator :: Loss: 4.9556236], 136.64507842063904s\n",
            "Batch: 100, [Discriminator :: Loss: 0.04384481], [ Generator :: Loss: 5.759879], 0.5040087699890137s\n",
            "Batch: 200, [Discriminator :: Loss: 0.045478866], [ Generator :: Loss: 6.803554], 0.40709662437438965s\n",
            "Batch: 300, [Discriminator :: Loss: 0.122778915], [ Generator :: Loss: 4.388658], 0.16187095642089844s\n",
            "Batch: 400, [Discriminator :: Loss: 0.11433923], [ Generator :: Loss: 4.978904], 0.16317391395568848s\n",
            "Epoch: 24, [Discriminator :: Loss: 0.043316394], [ Generator :: Loss: 5.2150946], 138.43839573860168s\n",
            "Batch: 100, [Discriminator :: Loss: 0.030530743], [ Generator :: Loss: 5.5484695], 0.16250228881835938s\n",
            "Batch: 200, [Discriminator :: Loss: 0.010237075], [ Generator :: Loss: 8.123643], 0.16197657585144043s\n",
            "Batch: 300, [Discriminator :: Loss: 0.029188517], [ Generator :: Loss: 6.129526], 0.1603090763092041s\n",
            "Batch: 400, [Discriminator :: Loss: 0.043414623], [ Generator :: Loss: 3.5531623], 0.2328488826751709s\n",
            "Batch: 500, [Discriminator :: Loss: 0.03351363], [ Generator :: Loss: 5.1937494], 0.14530110359191895s\n",
            "Epoch: 25, [Discriminator :: Loss: 0.011558432], [ Generator :: Loss: 6.6239114], 146.10414028167725s\n",
            "Batch: 100, [Discriminator :: Loss: 0.009814816], [ Generator :: Loss: 4.019529], 0.5083611011505127s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JExTvl7wIWFR",
        "colab_type": "text"
      },
      "source": [
        "# 学習結果\n",
        "\n",
        "## Epoch 1\n",
        "![alt text](https://drive.google.com/uc?id=1-G76knnWkU-CCnDFF8vaCskcWZFPpbMz)\n",
        "\n",
        "## Epoch 10\n",
        "![alt text](https://drive.google.com/uc?id=11l7mlpUW1SMKpo69us-fc1FZ7U34OrmP)\n",
        "\n",
        "## Epoch 20\n",
        "![alt text](https://drive.google.com/uc?id=15E4p2BeY7bzLBTZEweq4P3SkcLe-9VEQ)\n",
        "\n",
        "## Epoch 30\n",
        "![alt text](https://drive.google.com/uc?id=181o0lskvCHSCZ6de-LjNSdshAE2lQGnH)\n",
        "\n",
        "## Epoch 40\n",
        "![alt text](https://drive.google.com/uc?id=1Aa9aaSoFa9BnbIjQ0jLxh-PUYMaI0z4Q)\n",
        "\n",
        "## Epoch 50\n",
        "![alt text](https://drive.google.com/uc?id=1DEwm8NZxqALTWz83z70MQV3b29CIwLzH)\n",
        "\n",
        "## Epoch 60\n",
        "![alt text](https://drive.google.com/uc?id=1GKt6h9MYz9xV2L8YFg12Um8TCGZLQIiv)\n",
        "\n",
        "## Epoch 64\n",
        "![alt text](https://drive.google.com/uc?id=1HjSAMIo7IQwhH1KrRkUt-Vd6yHKcbnMQ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1mnBXIIPRRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}